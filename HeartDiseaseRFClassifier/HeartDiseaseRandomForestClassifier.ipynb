{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Random Forest Classifier\n",
    "\n",
    "This notebook implements a Random Forest Classifier to predict heart disease based on various features from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the Data Ready\n",
    "\n",
    "This step involves loading the dataset, splitting it into features and labels, and preparing the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "heart_disease = pd.read_csv('heart.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "# heart_disease.head()\n",
    "\n",
    "heart_disease.describe().T\n",
    "# The target column indicates whether the patient has heart disease (target=1) or not (target=0).\n",
    "# This is our 'label' column\n",
    "\n",
    "# Feature Selection\n",
    "# Create X (all the feature columns)\n",
    "X = heart_disease.drop('target', axis=1)\n",
    "\n",
    "# Create y (the target column)\n",
    "y = heart_disease['target']\n",
    "\n",
    "# Check the data\n",
    "X.head()\n",
    "# Check the head and the value counts of the labels\n",
    "y.head(), y.value_counts()\n",
    "\n",
    "# Split data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=43)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choose the Model and Hyperparameters\n",
    "\n",
    "In this step, we will choose the Random Forest model and view its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# View the current hyperparameters\n",
    "clf.get_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fit the Model with the Data\n",
    "\n",
    "This step involves fitting the model to the training data and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with training data\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "X_test.head()\n",
    "\n",
    "# Using the model to make predictions\n",
    "y_preds = clf.predict(X=X_test)\n",
    "y_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Model\n",
    "\n",
    "In this step, we will evaluate the model's performance using accuracy scores, classification reports, and confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score is a fraction of the correct predictions out of the total predictions\n",
    "train_acc = clf.score(X=X_train, y=y_train)\n",
    "print(f'Accuracy (Training Dataset): {train_acc * 100}%')\n",
    "test_acc = clf.score(X=X_test, y=y_test)\n",
    "print(f'Accuracy (Testing Dataset): {test_acc * 100}%')\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Create a classification report\n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_preds)\n",
    "conf_mat\n",
    "\n",
    "# Compute the accuracy score (same as the score() method for classifiers)\n",
    "accuracy_score(y_test, y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Hyperparameter Tuning\n",
    "\n",
    "In this step, we will experiment with different numbers of estimators and use cross-validation to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "for i in range(100, 200, 10):\n",
    "    print(f'Trying model with {i} estimators ...')\n",
    "    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "    print(f'Model accuracy on test set: {model.score(X_test, y_test) * 100:.2f}%')\n",
    "    print('')\n",
    "\n",
    "# Let's use sklearn.model_selection.cross_val_score to measure the results across 5 different train and test sets.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# With cross-validation\n",
    "np.random.seed(42)\n",
    "for i in range(100, 200, 10):\n",
    "    print(f'Trying model with {i} estimators...')\n",
    "    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "    \n",
    "    # Measure the model score on a single train/test split\n",
    "    model_score = model.score(X_test, y_test)\n",
    "    print(f'Model accuracy on single test set split: {model_score * 100:.2f}%')\n",
    "    \n",
    "    # Measure the mean cross-validation score across 5 different train and test splits\n",
    "    cross_val_mean = np.mean(cross_val_score(model, X, y, cv=5))\n",
    "    print(f'5-fold cross-validation score: {cross_val_mean * 100:.2f}%')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Saving\n",
    "\n",
    "In this step, we will save the trained model using both pickle and joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save an existing model to file\n",
    "pickle.dump(model, open('random_forest_model_1.pkl', 'wb'))\n",
    "\n",
    "# Load a saved pickle model and evaluate it\n",
    "# Loaded pickle model prediction score: (modelscore)%\n",
    "# However, for larger models, it may be more efficient to use Joblib.\n",
    "from joblib import dump, load\n",
    "\n",
    "# Save a model using joblib\n",
    "dump(model, 'random_forest_model_1.joblib')\n",
    "# Load a saved joblib model and evaluate it\n",
    "loaded_joblib_model = load('random_forest_model_1.joblib')\n",
    "print(f'Loaded joblib model prediction score: {loaded_joblib_model.score(X_test, y_test) * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</create_file>
